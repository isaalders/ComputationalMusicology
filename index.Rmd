---
title: "Computational Musicology"
author: "Isa Alders"
date: "Period 4"
output: 
    flexdashboard::flex_dashboard:
      storyboard: true
      theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(spotifyr)
spotifyr::get_spotify_access_token()
library(dplyr)
library(flexdashboard)
library(plotly)
library(compmus)
library(tidymodels)
library(ggdendro)
library(heatmaply)
```

### The similarity between my playlist and my mothers 

Growing up I've always listened to a lot of music at home. My parents are all big on music, especially my mom and my stepdad. Before the covid pandemic they used to go to concerts at least twice a month, and listen to music all day (this they still do). We as a family have gone to multiple concerts together, and always send in our lists for the top 2000 (an annual radio show where the listeners can vote for their favourite songs). Me and my mom have noticed that our lists always have many of the same songs on there. Now I actually had the opportunity, I have researched the similarities in the music we listen to. I did this by comparing my "Your Top Songs 2020", "Your Top Songs 2019" and "Your Top Songs 2018" to the ones of my parents account. These are lists Spotify creates for you. They contain the 100 songs you listened to most that specific year. Because some playlists contained duplicates, the total of songs didn't add up to 600 songs, but a little under that. 

I created two playlists in total. One with my three playlists combined named "Isa", and one with my parents three playlists combined named "MH". Furthermore, I examine a few mutual favourites on track-level features. This will help me understanding the features of specific songs we both like. 

```{r playlists, echo=FALSE}
isa <- get_playlist_audio_features("", "2JhcfrBpNKGKB5vut5PIY1")
mh <- get_playlist_audio_features("", "5DCCNsnEiJzi3uzhkMNdkG")


all_music <-
  bind_rows(
    isa %>% mutate(category = "Isa"),
    mh %>% mutate(category = "MH")
  )

```


### Comparing the features side by side {data-commentary-width=305}

```{r combination, echo=FALSE}
combination <- all_music %>%                    # Start with awards.
  ggplot(aes(x = valence, y = energy, size = instrumentalness, col = loudness, label = track.name)) +
    geom_point(alpha = 0.6) +
    facet_wrap(~ category) +
    scale_size_continuous(      # Fine-tune the sizes of each point.
      trans = "exp",            # Use an exp transformation to emphasise loud.
      guide = "legend"
    ) +
  scale_x_continuous(         # Fine-tune the x axis.
      limits = c(0, 1),
      breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
      limits = c(0, 1),
      breaks = c(0, 0.50, 1),
  ) +
  theme_light() +           # Use a simpler theme.
    labs(                       # Make the titles nice.
      x = "Valence",
      y = "Energy",
      colour = "Loudness",
      size = "Instrumentalness"
    ) 
    
ggplotly(combination)


```

*** 

In these plots you can see the **energy** plotted against the **valence**. **Color** is used to distinguish the **loudness** of the song. The **size** of the dot shows the **instrumentalness** of the specific song. The two plots are seperated to show the plot of my own playlist on the left, and the music of my parents playlist on the right. The two low-energy and low-valence outliers in my playlist are songs I listen to while studying, both from Ludovico Einaudi. 

Something that did suprise me is the fact that there is not much difference in the loudness between the songs. This suprises me especially because there are a lot of loud, classical rock songs in the playlist of my parents. I don't really know what caused this, but maybe it's just the way spotify implements loudness. You can see that in general, the songs are all quite evenly distributed, so it's hard to draw any real conclusions from this plot.

### Are my favourites more danceable than my mothers? 

```{r combination 2.0, echo=FALSE}

all_music %>%
  ggplot(aes(x = energy, fill=category)) +
  geom_histogram() +
  facet_wrap(~ category, scales="free_y")+
  scale_x_continuous(
    limits=c(0,1),
    breaks=c(0,0.5,1)
  )+
  labs(fill='Playlist owner', title='Count of energy per playlist owner') +
  xlab('Energy') +
  ylab("Count")

all_music %>%
  ggplot(aes(x = tempo,  fill=category)) +
  geom_histogram() +
  facet_wrap(~ category)+
  scale_x_continuous(
  limits=c(0,250),
  breaks=c(0,125,250)
  )+
  labs(fill='Playlist owner', title='Count of tempo per playlist owner') +
  xlab('Tempo')+
  ylab("Count")


all_music %>%
  ggplot(aes(x = danceability, fill=category)) +
  geom_histogram() +
  facet_wrap(~ category, scales="free_y")+
  scale_x_continuous(
  limits=c(0,1),
  breaks=c(0,0.5,1)
  )+
  labs(fill='Playlist owner', title='Count of danceability per playlist owner') +
  xlab('Danceability') +
  ylab("Count")
```

*** 

On the left, Iâ€™ve made plots that compare the energy, the tempo and the danceability of my playlist with my mothers. Here you can easily compare the variables. 

First, we have the **energy**. You can see that in the MH playlists, the energy is a little more **centred to the right**, and in the Isa playlists, the energy is more **centred in the middle**. We both barely have songs in our playlists that are very low in energy. 

Next, we can look at the **tempo** feature. These plots are quite similar. Again, the mean tempo of the MH playlists is probably	a **little bit higher** than the Isa playlist, but overall, very similar. You can also see that the peak in the Isa graph is higher, but this is normal, because the Isa playlists has more songs. 

Than to conclude, we have the **danceability** feature. These are again very similar. Both graphs have a peak a **little to the right**, the only difference is is that the MH playlists have a small peak a little to the right. 

### Zooming in on a mutual favourite {data-commentary-width=500}

```{r, echo=FALSE}
fleetwood_mac <-
  get_tidy_audio_analysis("5e9TFTbltYBg2xThimr0rU") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

fleetwood_mac_plot <- fleetwood_mac %>%
  mutate(pitches = map(pitches, compmus_normalise, "chebyshev")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
fleetwood_mac_plot
```

*** 

On the left we can see a chromagram of *The Chain* by *Fleetwood Mac*. This is a song both my mom and I have in our playlists. You could say this song is one of the classics in our household, and I thought it would be nice to zoom into this song. What are typical features of a song we both like? We can see that **G#, Ab** and **E** are quite present in this song. 


<object data="https://open.spotify.com/embed/track/5e9TFTbltYBg2xThimr0rU" width="280" height="140">
    <embed src="https://open.spotify.com/embed/track/5e9TFTbltYBg2xThimr0rU" width="280" height="140"></embed>
</object>


### Inspecting the chords of the common favourite

```{r echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

fleetwood_mac2 <-
  get_tidy_audio_analysis("5e9TFTbltYBg2xThimr0rU") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

fleetwood_mac2 %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "manhattan",  # Try different distance metrics
    norm = "euclidean"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

On the left we can see a chordogram of the same song as the last slide: The Chain by FLeetwood Mac. When we look at this chordogram, it seems that **G major** and **D minor** are most present in the song. 

### The structure of Do I Wanna Know? in a self similarity matrix {data-commentary-width=305}

```{r, echo=FALSE}
arctic_monkeys <-
  get_tidy_audio_analysis("5FVd6KXrgO9B3JPmC8OPst") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  arctic_monkeys %>%
    compmus_self_similarity(pitches, "aitchison") %>%
    mutate(d = d / max(d), type = "Chroma"),
  arctic_monkeys %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

*** 

These are the self-similarity matrices for **Do I Wanna Know** by the **Arctic Monkeys**, which is also a song that is in both playlits. Just like Fleetwood Mac, the Arctic Monkeys are a true family favourite and we've been to multiple concerts of them together. In the timbre self-similarity matrix, a yellow cross is visible. I haven't figured out what this might be by listening to the song. The song is quite slow and steady. The two matrices are both at bars level, and I used the **aitchison** and **euclidean** normalisation, respectively. The structure of the song is not clearly visible in the Chroma SSM. I think this also has to do with the fact that it's quite a steady song with not many changes. 

<object data="https://open.spotify.com/embed/track/5FVd6KXrgO9B3JPmC8OPst" width="280" height="140">
    <embed src="https://open.spotify.com/embed/track/5FVd6KXrgO9B3JPmC8OPst" width="280" height="140"></embed>
</object>

### Do I Wanna Know? is a very consistent song

![Tempogram](/Users/Isa/Downloads/000003 (1).png)

---   

Here we can see the tempogram of Do I Wanna Know by the Arctic Monkeys. This song is, as stated before, a  typical one for both of our playlists. I chose this song for the tempogram, because, like I said, in my opinion, this song is very slow and steady, and very much in the same tempo. This is also visible in the tempogram, since there is a steady line around 350 BPM, and around 175 BPM. 

<object data="https://open.spotify.com/embed/track/5FVd6KXrgO9B3JPmC8OPst" width="280" height="140">
    <embed src="https://open.spotify.com/embed/track/5FVd6KXrgO9B3JPmC8OPst" width="280" height="140"></embed>
</object>

### Can a classifier predict which song is my favourite or my mothers favourite? 

```{r, echo=FALSE}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  
```

```{r, echo=FALSE}
Isa <- 
  get_playlist_audio_features("spotify", "2JhcfrBpNKGKB5vut5PIY1")
MH <- get_playlist_audio_features("spotify", "5DCCNsnEiJzi3uzhkMNdkG")

compared <-
  bind_rows(
    Isa %>% mutate(playlist = "Isa") %>% slice_head(n = 30),
    MH %>% mutate(playlist = "MH") %>% slice_head(n = 30)
  ) 
```

```{r,echo=FALSE}
indie_features <-
  compared %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```


```{r,echo=FALSE}
indie_recipe <-
  recipe(
    playlist ~
      danceability +
      speechiness +
      acousticness +
      valence +
      duration +
      
      `F` +
      c02 + c04 + c05 ,
    data = indie_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
```

```{r,echo=FALSE}
indie_cv <- indie_features %>% vfold_cv(10)
```
```{r,echo = FALSE}
knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>% 
  set_engine("kknn")
indie_knn <- 
  workflow() %>% 
  add_recipe(indie_recipe) %>% 
  add_model(knn_model) %>% 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```
```{r,echo=FALSE}
indie_knn %>% get_conf_mat() %>% autoplot(type = "mosaic")
```



***

In the previous part of this storyboard, the differences between the two playlists have been shown. Is it possible to classify a song to which list it belongs? I used a random forest to answer this question. On the next page, the most important features are shown. The most important features turned out to be **danceability**, **speechiness** and **duration**. These features made distinguishing the two lists the easiest. We can see in the plot on the left that most of the songs are classified in the right playlist. Only less than 20% is not classified in the right playlist. The **precision** for Isa is 0.846, and for MH 0.765. The **recall** for Isa is 0.733 and for MH 0.867. 




### Importance of features for clustering 

```{r echo=FALSE}
forest_model <-
  rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("ranger", importance = "impurity")
indie_forest <- 
  workflow() %>% 
  add_recipe(indie_recipe) %>% 
  add_model(forest_model) %>% 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```
```{r echo=FALSE}
workflow() %>% 
  add_recipe(indie_recipe) %>% 
  add_model(forest_model) %>% 
  fit(indie_features) %>% 
  pluck("fit", "fit", "fit") %>%
  ranger::importance() %>% 
  enframe() %>% 
  mutate(name = fct_reorder(name, value)) %>% 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")
```

### Are our favourites indeed very similar? 

Looking at the rest of the portfolio, we can say that the music of both of our favourite playlists are pretty similar. There are a few outliers and a few differences, like some hard rock in MH, and some study music in Isa, but overall the tempo, danceability and energy are pretty similar. The classifier also performs pretty well, so this also gives us an indication.